nohup: ignoring input
Building prefix dict from the default dictionary ...
Loading model from cache /tmp/jieba.cache
Loading model cost 0.613 seconds.
Prefix dict has been built succesfully.
WARNING:tensorflow:From /usr/anaconda2/envs/py3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/preprocessing/text.py:203: tokenizer (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.
Instructions for updating:
Please use tensorflow/transform or tf.data.
2020-02-08 19:49:55.505885: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2020-02-08 19:49:57.081567: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53
pciBusID: 0000:8a:00.0
totalMemory: 15.78GiB freeMemory: 15.37GiB
2020-02-08 19:49:57.081637: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2020-02-08 19:49:57.574650: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-02-08 19:49:57.574699: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2020-02-08 19:49:57.574707: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2020-02-08 19:49:57.575170: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14874 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:8a:00.0, compute capability: 7.0)

TRAIN_PATH=clinic_data/train_has_ID_all_1_1000(new).txt
TEST_PATH=clinic_data/test_has_ID_all(new).txt
MAX_SENTENCE_LENGTH=300
DEV_SAMPLE_PERCENTAGE=0.1
EMBEDDING_PATH=None
EMBEDDING_DIM=150
EMB_DROPOUT_KEEP_PROB=0.7
HIDDEN_SIZE=100
RNN_DROPOUT_KEEP_PROB=0.7
DESC=
DROPOUT_KEEP_PROB=0.5
L2_REG_LAMBDA=1e-05
BATCH_SIZE=10
NUM_EPOCHS=100
DISPLAY_EVERY=10
EVALUATE_EVERY=100
NUM_CHECKPOINTS=5
LEARNING_RATE=1.0
DECAY_RATE=0.9
CHECKPOINT_DIR=/home/ontoweb/0+J/Attention-Based-BiLSTM-relation-extraction1.1/runs/1572576540/checkpoints
ALLOW_SOFT_PLACEMENT=True
LOG_DEVICE_PLACEMENT=False
GPU_ALLOW_GROWTH=True

clinic_data/test_has_ID_all(new).txt
max sentence length = 483

[0 0 0 ... 1 1 1]
===the prediction result===
	0	1	2	3	4	5
0	2452	48	37	215	56	13
1	67	2383	33	54	268	16
2	36	34	87	25	30	4
3	156	15	17	185	15	4
4	20	142	14	13	201	2
5	27	25	7	2	2	3
[86.9, 84.5, 40.300000000000004, 47.199999999999996, 51.300000000000004, 4.5] [88.9, 90.0, 44.6, 37.4, 35.099999999999994, 7.1] [87.9, 87.2, 42.3, 41.7, 41.7, 5.5]
beforeacc:86.9%,recall:88.9%,f1:87.9%
afteracc:84.5%,recall:90.0%,f1:87.2%
simultaneousacc:40.300000000000004%,recall:44.6%,f1:42.3%
includeacc:47.199999999999996%,recall:37.4%,f1:41.7%
be_includedacc:51.300000000000004%,recall:35.099999999999994%,f1:41.7%
vagueacc:4.5%,recall:7.1%,f1:5.5%
acc_avg:52.5%,recall_avg:50.5%,f1:0.7917412045319022%
modelFile:/home/ontoweb/0+J/Attention-Based-BiLSTM-relation-extraction1.1/runs/1572576540/checkpoints
